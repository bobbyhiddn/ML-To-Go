# Week 5: Transformer Models

## Learning Objectives
- Understand transformer architecture and attention mechanisms
- Use pre-trained models (BERT, RoBERTa, DistilBERT)
- Implement document classification with transformers
- Learn about tokenization and model inputs

## Topics Covered
- Transformer architecture and self-attention
- Pre-trained language models
- Hugging Face transformers library
- Model inference and predictions

## Exercises
1. Load pre-trained transformer models
2. Implement document classification with BERT
3. Compare different transformer models
4. Analyze attention weights and model interpretability

## Resources
- Hugging Face Transformers Documentation
- BERT Paper and Implementation
- Attention Is All You Need Paper
